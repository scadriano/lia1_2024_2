{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu5OLDcwuSoe"
      },
      "source": [
        "# **Construindo um Modelo com Tensorflow -  ğŸ¶ ğŸ±!**\n",
        "\n",
        "**Problema: uma imagem, dizer se Ã© um dog ou um cat.**\n",
        "\n",
        "Computer Vision; Deep Learning; Machine Learning; Artificial Inteligence! Nada disso faz sentido sem dados, muitos dados (Big Data! ğŸš€). Para isso, teremos:\n",
        "\n",
        "* **Treinamento**: 25.000 imagens nomeadas: 12.500 de dogs e 12.500 de cats.\n",
        "* **Teste**: 1.000 imagens de dogs e cats.\n",
        "\n",
        "Usa-se os dados de treino para treinar o algoritmo e entÃ£o criar o modelo preditivo. Usa-se os dados de teste para confirmar o desempenho do modelo preditivo jÃ¡ treinado, ou seja, apresenta-se ao modelo preditivo dados que ele nÃ£o viu durante o treinamento, a fim de garantir que ele seja capaz de fazer previsÃµes.\n",
        "\n",
        "Por fim, o modelo de duas camadas de convoluÃ§Ã£o seguidas de pooling, a camada de flattening, e as camadas totalmente conectadas (Dense), com a funÃ§Ã£o de ativaÃ§Ã£o sigmoid para a saÃ­da binÃ¡ria.\n",
        "\n",
        "**NÃ£o hÃ¡ mÃ¡gica. HÃ¡ matemÃ¡tica!** ğŸ§™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv1N0nRuxwL-"
      },
      "source": [
        "**Fonte de dados**\n",
        "\n",
        "O Kaggle oferece diversos datasets pÃºblicos que podem ser usados para vocÃª desenvolver seus projetos e incluir no seu portfÃ³lio, uma excelente forma de demonstrar suas habilidades em Data Science e Machine Learning. Usaremos como fonte de dados, o famoso [dataset Dogs and Cats](https://www.kaggle.com/c/dogs-vs-cats/data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B80jFLJAym_v"
      },
      "source": [
        "**Carregando os dados de Treino e Teste**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN2T6cv24Y6A"
      },
      "source": [
        "**Construindo a Rede Neural Convolucional**\n",
        "\n",
        "O Keras Ã© uma biblioteca do TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LNpCfaa8NfdY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eb0TUqkQNjaI"
      },
      "outputs": [],
      "source": [
        "import keras as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XJldwmiJ4hHA"
      },
      "outputs": [],
      "source": [
        "# Importar K e suas funÃ§Ãµes necessÃ¡rias\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MYZrHMd85PqQ"
      },
      "outputs": [],
      "source": [
        "# Inicializando a Rede Neural Convolucional\n",
        "classifier = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FrympYJ96BWg"
      },
      "outputs": [],
      "source": [
        "# Definindo a arquitetura do modelo\n",
        "classifier = Sequential()\n",
        "# Usando Input como a primeira camada\n",
        "classifier.add(Input(shape=(64, 64, 3)))  # Definindo a forma da entrada\n",
        "classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(Conv2D(64, (3, 3), activation='relu'))  # Camada convolucional adicional\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(Conv2D(128, (3, 3), activation='relu'))  # Camada convolucional adicional\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(Flatten())\n",
        "classifier.add(Dense(units=128, activation='relu'))\n",
        "classifier.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compilando o modelo\n",
        "classifier.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['binary_accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1l-VgZ78NNS"
      },
      "source": [
        "**PrÃ©-processamento**\n",
        "\n",
        "Fazer prÃ©-processamento nos dados, em nosso caso as imagens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T8au9i8V7mDZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Criar o objeto com as regras de prÃ©-processamento\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "BLiOSL6h7vAI",
        "outputId": "fdd4c566-a918-409a-831b-f0c3e1875be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# PrÃ©-processamento das imagens de treino\n",
        "training_set = train_datagen.flow_from_directory(r'C:\\Users\\ryanr\\Documents\\Python\\datasets\\dogs-vs-cats\\train',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 64,\n",
        "                                                 shuffle=True,\n",
        "                                                 seed=198,\n",
        "                                                 class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ephZE1Y59LKq"
      },
      "source": [
        "**Treinamento do Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3KVRGudo9No6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ryanr\\Documents\\Python\\venvs\\dogs-or-cats\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 609ms/step - binary_accuracy: 0.4950 - categorical_accuracy: 0.5635 - loss: 0.7060\n",
            "Epoch 2/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 599ms/step - binary_accuracy: 0.5078 - categorical_accuracy: 0.3211 - loss: 0.6951\n",
            "Epoch 3/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 519ms/step - binary_accuracy: 0.5564 - categorical_accuracy: 0.6328 - loss: 0.6869\n",
            "Epoch 4/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 537ms/step - binary_accuracy: 0.5391 - categorical_accuracy: 0.5206 - loss: 0.6891\n",
            "Epoch 5/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 474ms/step - binary_accuracy: 0.5561 - categorical_accuracy: 0.5241 - loss: 0.6838\n",
            "Epoch 6/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 227ms/step - binary_accuracy: 0.5549 - categorical_accuracy: 0.3804 - loss: 0.6833\n",
            "Epoch 7/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ryanr\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(value)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 664ms/step - binary_accuracy: 0.5648 - categorical_accuracy: 0.4837 - loss: 0.6798\n",
            "Epoch 8/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 420ms/step - binary_accuracy: 0.5843 - categorical_accuracy: 0.5067 - loss: 0.6707\n",
            "Epoch 9/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 366ms/step - binary_accuracy: 0.5713 - categorical_accuracy: 0.4767 - loss: 0.6762\n",
            "Epoch 10/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 346ms/step - binary_accuracy: 0.5957 - categorical_accuracy: 0.4714 - loss: 0.6636\n",
            "Epoch 11/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 390ms/step - binary_accuracy: 0.5963 - categorical_accuracy: 0.5346 - loss: 0.6590\n",
            "Epoch 12/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 180ms/step - binary_accuracy: 0.6185 - categorical_accuracy: 0.4517 - loss: 0.6535\n",
            "Epoch 13/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 338ms/step - binary_accuracy: 0.5967 - categorical_accuracy: 0.4686 - loss: 0.6649\n",
            "Epoch 14/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 308ms/step - binary_accuracy: 0.6094 - categorical_accuracy: 0.4502 - loss: 0.6508\n",
            "Epoch 15/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 296ms/step - binary_accuracy: 0.6295 - categorical_accuracy: 0.4621 - loss: 0.6404\n",
            "Epoch 16/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 307ms/step - binary_accuracy: 0.6155 - categorical_accuracy: 0.4401 - loss: 0.6486\n",
            "Epoch 17/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 312ms/step - binary_accuracy: 0.6418 - categorical_accuracy: 0.4204 - loss: 0.6345\n",
            "Epoch 18/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 190ms/step - binary_accuracy: 0.6452 - categorical_accuracy: 0.5275 - loss: 0.6314\n",
            "Epoch 19/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 369ms/step - binary_accuracy: 0.6322 - categorical_accuracy: 0.5625 - loss: 0.6386\n",
            "Epoch 20/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 314ms/step - binary_accuracy: 0.6479 - categorical_accuracy: 0.4414 - loss: 0.6183\n",
            "Epoch 21/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 298ms/step - binary_accuracy: 0.6741 - categorical_accuracy: 0.5682 - loss: 0.6146\n",
            "Epoch 22/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 302ms/step - binary_accuracy: 0.6527 - categorical_accuracy: 0.4119 - loss: 0.6249\n",
            "Epoch 23/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 378ms/step - binary_accuracy: 0.6482 - categorical_accuracy: 0.5167 - loss: 0.6280\n",
            "Epoch 24/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 167ms/step - binary_accuracy: 0.6596 - categorical_accuracy: 0.5984 - loss: 0.6133\n",
            "Epoch 25/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 300ms/step - binary_accuracy: 0.6601 - categorical_accuracy: 0.5131 - loss: 0.6050\n",
            "Epoch 26/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 344ms/step - binary_accuracy: 0.6818 - categorical_accuracy: 0.4789 - loss: 0.5996\n",
            "Epoch 27/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 310ms/step - binary_accuracy: 0.6789 - categorical_accuracy: 0.5749 - loss: 0.6010\n",
            "Epoch 28/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 358ms/step - binary_accuracy: 0.6801 - categorical_accuracy: 0.4527 - loss: 0.5889\n",
            "Epoch 29/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 311ms/step - binary_accuracy: 0.6828 - categorical_accuracy: 0.5872 - loss: 0.6025\n",
            "Epoch 30/30\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 166ms/step - binary_accuracy: 0.6471 - categorical_accuracy: 0.3315 - loss: 0.6247\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x21f63450fb0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Executando o treinamento\n",
        "classifier.fit(training_set,\n",
        "               steps_per_epoch=70,\n",
        "               epochs=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl1MFhzd9aph"
      },
      "source": [
        "Treinamento concluÃ­do com sucesso! ğŸ’ª Observe se ao final de cada Ã©poca a acurÃ¡cia aumenta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OIAQ0GU-WrK"
      },
      "source": [
        "**Teste do Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cats': 0, 'dogs': 1}\n"
          ]
        }
      ],
      "source": [
        "print(training_set.class_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testar o modelo treinado com TODAS AS IMAGENS que ele ainda nÃ£o viu. Por fim, verificamos o resultado da acurÃ¡cia total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 12500 images belonging to 1 classes.\n",
            "\u001b[1m  2/391\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m21s\u001b[0m 55ms/step "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ryanr\\Documents\\Python\\venvs\\dogs-or-cats\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 65ms/step\n",
            "ConfianÃ§a mÃ©dia do modelo: 50.95%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# AvaliaÃ§Ã£o do Modelo\n",
        "# Configurando o gerador de dados de teste (sem data augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    r'C:\\Users\\ryanr\\Documents\\Python\\datasets\\dogs-vs-cats\\test1',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode=None,  # Sem labels, logo, a mÃ©trica de test_loss nÃ£o serÃ¡ calculada.\n",
        "    shuffle=True  # Para manter a ordem das imagens\n",
        ")\n",
        "\n",
        "# Obtendo as prediÃ§Ãµes do modelo\n",
        "predictions = classifier.predict(test_set)\n",
        "\n",
        "# Calculando a confianÃ§a das prediÃ§Ãµes\n",
        "confidence_scores = np.max(predictions, axis=1)  # Para um modelo binÃ¡rio, isso dÃ¡ a confianÃ§a da prediÃ§Ã£o\n",
        "\n",
        "# Calculando a confianÃ§a mÃ©dia\n",
        "mean_confidence = np.mean(confidence_scores)\n",
        "\n",
        "# Imprimindo a confianÃ§a mÃ©dia\n",
        "print(f'ConfianÃ§a mÃ©dia do modelo: {mean_confidence * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goRr2pHz-qrp"
      },
      "source": [
        "**O modelo recebeu uma imagem que nunca tinha visto antes e com base no que aprendeu durante o treinamento, foi capaz de classificar.**\n",
        "\n",
        "Convertemos a imagem de teste em um vetor de pixels e apresentamos ao modelo.\n",
        "O modelo compara o vetor da imagem de teste com seus pesos e entÃ£o emite a classificaÃ§Ã£o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfsCG3KP9kwr"
      },
      "source": [
        "**Melhorias adicionais para este modelo:**\n",
        "\n",
        "*   Aumentar o nÃºmero de Ã©pocas para 25 para uma aprendizagem mais profunda.\n",
        "*   aumentar o redimensionamento da imagem de 64x64 para 256x256.\n",
        "*   Aumentar o tamanho do lote de 32 para 64.\n",
        "*   Alterar a arquitetura da rede incluindo mais uma camada convolucional.\n",
        "*   Avaliar outras mÃ©tricas do modelo e ajustar os hiperparÃ¢metros de acordo.\n",
        "*   Experimentar outros algoritmos de otimizaÃ§Ã£o.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWz1StcE-MZz"
      },
      "source": [
        "Fim! ğŸ”¥"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
