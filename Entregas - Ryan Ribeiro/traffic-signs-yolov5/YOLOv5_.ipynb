{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryan-ribeiro/lia1_2024_2/blob/main/Entregas%20-%20Ryan%20Ribeiro/traffic-signs-yolov5/YOLOv5_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLOv5 Classification Tutorial\n",
        "\n",
        "YOLOv5 supports classification tasks too. This is the official YOLOv5 classification notebook tutorial. YOLOv5 is maintained by [Ultralytics](https://github.com/ultralytics/yolov5).\n",
        "\n",
        "This notebook covers:\n",
        "\n",
        "*   Inference with out-of-the-box YOLOv5 classification on ImageNet\n",
        "*  [Training YOLOv5 classification](https://blog.roboflow.com//train-YOLOv5-classification-custom-data) on custom data\n",
        "\n",
        "*Looking for custom data? Explore over 66M community datasets on [Roboflow Universe](https://universe.roboflow.com).*\n",
        "\n",
        "This notebook was created with Google Colab. [Click here](https://colab.research.google.com/drive/1FiSNz9f_nT8aFtDEU3iDAQKlPT8SCVni?usp=sharing) to run it."
      ],
      "metadata": {
        "id": "5GYQX3of4QiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Pull in respective libraries to prepare the notebook environment."
      ],
      "metadata": {
        "id": "-PJ8vlYXbWtN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIM7fOwm8A7l",
        "outputId": "3ee593f7-f204-4334-df12-1e51df24c31f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ğŸš€ v7.0-377-g24ee2801 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 32.3/112.6 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Infer on ImageNet\n",
        "\n",
        "To demonstrate YOLOv5 classification, we'll leverage an already trained model. In this case, we'll download the ImageNet trained models pretrained on ImageNet using YOLOv5 Utils."
      ],
      "metadata": {
        "id": "i_DrUi2nmF40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.downloads import attempt_download\n",
        "\n",
        "p5 = ['n', 's', 'm', 'l', 'x']  # P5 models\n",
        "cls = [f'{x}-cls' for x in p5]  # classification models\n",
        "\n",
        "for x in cls:\n",
        "    attempt_download(f'weights/yolov5{x}.pt')"
      ],
      "metadata": {
        "id": "o2scLEh6EYnL",
        "outputId": "724c3b47-2f15-4656-9774-d0ce0404fc55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-cls.pt to weights/yolov5n-cls.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.87M/4.87M [00:00<00:00, 80.3MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to weights/yolov5s-cls.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.5M/10.5M [00:00<00:00, 85.2MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-cls.pt to weights/yolov5m-cls.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.9M/24.9M [00:00<00:00, 107MB/s] \n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-cls.pt to weights/yolov5l-cls.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.9M/50.9M [00:00<00:00, 109MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-cls.pt to weights/yolov5x-cls.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92.0M/92.0M [00:00<00:00, 104MB/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can infer on an example image from the ImageNet dataset."
      ],
      "metadata": {
        "id": "Fn2_a38DmZ2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download example image\n",
        "import requests\n",
        "image_url = \"https://i.imgur.com/OczPfaz.jpg\"\n",
        "img_data = requests.get(image_url).content\n",
        "with open('bananas.jpg', 'wb') as handler:\n",
        "    handler.write(img_data)"
      ],
      "metadata": {
        "id": "L9objhVHnS-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Infer using classify/predict.py\n",
        "!python classify/predict.py --weights ./weigths/yolov5s-cls.pt --source bananas.jpg"
      ],
      "metadata": {
        "id": "qqxF5pHCrLd3",
        "outputId": "d631facb-c978-43a2-e708-797f5b15e90f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['./weigths/yolov5s-cls.pt'], source=bananas.jpg, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=False, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 ğŸš€ v7.0-377-g24ee2801 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to weigths/yolov5s-cls.pt...\n",
            "100% 10.5M/10.5M [00:00<00:00, 92.4MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 117 layers, 5447688 parameters, 0 gradients, 11.4 GFLOPs\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/classify/predict.py\", line 241, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/classify/predict.py\", line 236, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/yolov5/classify/predict.py\", line 122, in run\n",
            "    for path, im, im0s, vid_cap, s in dataset:\n",
            "  File \"/content/yolov5/utils/dataloaders.py\", line 396, in __next__\n",
            "    im0 = cv2.imread(path)  # BGR\n",
            "  File \"/content/yolov5/utils/general.py\", line 1274, in imread\n",
            "    return cv2.imdecode(np.fromfile(filename, np.uint8), flags)\n",
            "cv2.error: OpenCV(4.10.0) /io/opencv/modules/imgcodecs/src/loadsave.cpp:813: error: (-215:Assertion failed) !buf.empty() in function 'imdecode_'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output, we can see the ImageNet trained model correctly predicts the class `banana` with `0.95` confidence."
      ],
      "metadata": {
        "id": "yQmj7IXqo3kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. (Optional) Validate\n",
        "\n",
        "Use the `classify/val.py` script to run validation for the model. This will show us the model's performance on each class.\n",
        "\n",
        "First, we need to download ImageNet."
      ],
      "metadata": {
        "id": "5EosQzyDCk3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # WARNING: takes ~20 minutes\n",
        "# !bash data/scripts/get_imagenet.sh --val"
      ],
      "metadata": {
        "id": "HwAYptjCq-C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # run the validation script\n",
        "# !python classify/val.py --weights ./weigths/yolov5s-cls.pt --data ../datasets/imagenet"
      ],
      "metadata": {
        "id": "CoHdKXWc8hrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows accuracy metrics for the ImageNet validation dataset including per class accuracy."
      ],
      "metadata": {
        "id": "r2coOcIjuzCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train On Custom Data\n",
        "\n",
        "To train on custom data, we need to prepare a dataset with custom labels.\n",
        "\n",
        "To prepare custom data, we'll use [Roboflow](https://roboflow.com). Roboflow enables easy dataset prep with your team, including labeling, formatting into the right export format, deploying, and active learning with a `pip` package.\n",
        "\n",
        "If you need custom data, there are over 66M open source images from the community on [Roboflow Universe](https://universe.roboflow.com).\n",
        "\n",
        "(For more guidance, here's a detailed blog on [training YOLOv5 classification on custom data](https://blog.roboflow.com/train-YOLOv5-classification-custom-data).)\n",
        "\n",
        "\n",
        "Create a free Roboflow account, upload your data, and label.\n",
        "\n",
        "![](https://s4.gifyu.com/images/fruit-labeling.gif)"
      ],
      "metadata": {
        "id": "9bXHHYeVDCXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Custom Dataset\n",
        "\n",
        "Next, we'll export our dataset into the right directory structure for training YOLOv5 classification to load into this notebook. Select the `Export` button at the top of the version page, `Folder Structure` type, and `show download code`.\n",
        "\n",
        "The ensures all our directories are in the right format:\n",
        "\n",
        "```\n",
        "dataset\n",
        "â”œâ”€â”€ train\n",
        "â”‚Â Â  â”œâ”€â”€ class-one\n",
        "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_123.jpg\n",
        "â”‚Â Â  â””â”€â”€ class-two\n",
        "â”‚Â Â      â”œâ”€â”€ IMG_456.jpg\n",
        "â”œâ”€â”€ valid\n",
        "â”‚Â Â  â”œâ”€â”€ class-one\n",
        "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_789.jpg\n",
        "â”‚Â Â  â””â”€â”€ class-two\n",
        "â”‚Â Â      â”œâ”€â”€ IMG_101.jpg\n",
        "â”œâ”€â”€ test\n",
        "â”‚Â Â  â”œâ”€â”€ class-one\n",
        "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_121.jpg\n",
        "â”‚Â Â  â””â”€â”€ class-two\n",
        "â”‚Â Â      â”œâ”€â”€ IMG_341.jpg\n",
        "```\n",
        "\n",
        "![](https://i.imgur.com/BF9BNR8.gif)\n",
        "\n",
        "\n",
        "Copy and paste that snippet into the cell below."
      ],
      "metadata": {
        "id": "Cu6-lrukD6Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure we're in the right directory to download our custom dataset\n",
        "import os\n",
        "os.makedirs(\"../datasets/\", exist_ok=True)\n",
        "%cd ../datasets/"
      ],
      "metadata": {
        "id": "6IIgJbP7G6Th",
        "outputId": "b06f5678-2bf4-44da-c23e-6eef530ea5b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REPLACE the below with your exported code snippet from above\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"Jbx8FKHjs8dAPgHYtV7X\")\n",
        "project = rf.workspace(\"hautique-x\").project(\"traffic-signs-detection-txg3k\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"folder\")\n"
      ],
      "metadata": {
        "id": "He6JwHIlG-W_",
        "outputId": "40207f16-ddbf-4256-a60c-d48767ba3fd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.48-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.54.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\n",
            "Downloading roboflow-1.1.48-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, python-dotenv, idna, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 python-dotenv-1.0.1 requests-toolbelt-1.0.0 roboflow-1.1.48\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Traffic-Signs-Detection-1 to folder:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 236196/236196 [00:04<00:00, 52601.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Traffic-Signs-Detection-1 in folder:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23726/23726 [00:02<00:00, 8114.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the dataset name to the environment so we can use it in a system call later\n",
        "dataset_name = dataset.location.split(os.sep)[-1]\n",
        "os.environ[\"DATASET_NAME\"] = dataset_name"
      ],
      "metadata": {
        "id": "wLQbThFICpn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train On Custom Data ğŸ‰\n",
        "Here, we use the DATASET_NAME environment variable to pass our dataset to the `--data` parameter.\n",
        "\n",
        "Note: we're training for 100 epochs here. We're also starting training from the pretrained weights. Larger datasets will likely benefit from longer training."
      ],
      "metadata": {
        "id": "-5z7Yv42FGrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../yolov5\n",
        "!python classify/train.py --model yolov5s-cls.pt --data $DATASET_NAME --epochs 100 --img 128 --pretrained weights/yolov5s-cls.pt"
      ],
      "metadata": {
        "id": "MXWTTN2BEaqe",
        "outputId": "c1b00b58-963d-497e-92cb-adf2c0ea3b2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "2024-10-20 20:40:06.632211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-20 20:40:06.651652: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-20 20:40:06.658111: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5s-cls.pt, data=Traffic-Signs-Detection-1, epochs=100, batch_size=64, imgsz=128, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5s-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ğŸš€ v7.0-377-g24ee2801 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, size=(128, 128), scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.6, 1.4), hue=(0.0, 0.0)), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, normalization='standard'), ToTensorV2(p=1.0, transpose_mask=False)\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to yolov5s-cls.pt...\n",
            "100% 10.5M/10.5M [00:00<00:00, 170MB/s]\n",
            "\n",
            "Model summary: 149 layers, 4227563 parameters, 4227563 gradients, 10.5 GFLOPs\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\n",
            "/content/yolov5/classify/train.py:201: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=cuda)\n",
            "Image sizes 128 train, 128 test\n",
            "Using 1 dataloader workers\n",
            "Logging results to \u001b[1mruns/train-cls/exp\u001b[0m\n",
            "Starting yolov5s-cls.pt training on Traffic-Signs-Detection-1 dataset with 43 classes for 100 epochs...\n",
            "\n",
            "     Epoch   GPU_mem  train_loss   test_loss    top1_acc    top5_acc\n",
            "  0% 0/323 [00:00<?, ?it/s]/content/yolov5/classify/train.py:222: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(enabled=cuda):  # stability issues when enabled\n",
            "     1/100    0.505G        3.14                             testing:   0% 0/8 [00:00<?, ?it/s]/content/yolov5/classify/val.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=device.type != \"cpu\"):\n",
            "     1/100    0.505G        3.14         3.1       0.161       0.511: 100% 323/323 [00:44<00:00,  7.18it/s]\n",
            "     2/100    0.558G        2.52        2.62       0.336       0.738: 100% 323/323 [00:41<00:00,  7.82it/s]\n",
            "     3/100    0.558G        1.92        1.58       0.667       0.963: 100% 323/323 [00:43<00:00,  7.40it/s]\n",
            "     4/100    0.558G        1.49        1.39       0.769       0.964: 100% 323/323 [00:42<00:00,  7.59it/s]\n",
            "     5/100    0.558G        1.32        1.16       0.824       0.989: 100% 323/323 [00:41<00:00,  7.86it/s]\n",
            "     6/100    0.558G        1.25        1.23       0.823       0.967: 100% 323/323 [00:43<00:00,  7.47it/s]\n",
            "     7/100    0.558G        1.18        1.09       0.861       0.995: 100% 323/323 [00:43<00:00,  7.49it/s]\n",
            "     8/100    0.558G        1.14        1.11       0.874       0.983: 100% 323/323 [00:42<00:00,  7.68it/s]\n",
            "     9/100    0.558G        1.12       0.984       0.907       0.987: 100% 323/323 [00:41<00:00,  7.81it/s]\n",
            "    10/100    0.558G         1.1        1.12        0.86       0.964: 100% 323/323 [00:42<00:00,  7.62it/s]\n",
            "    11/100    0.558G        1.06        1.11        0.88       0.974: 100% 323/323 [00:42<00:00,  7.56it/s]\n",
            "    12/100    0.558G        1.06        1.17       0.843       0.959: 100% 323/323 [00:40<00:00,  7.97it/s]\n",
            "    13/100    0.558G        1.04       0.919       0.919       0.998: 100% 323/323 [00:42<00:00,  7.53it/s]\n",
            "    14/100    0.558G        1.03       0.934       0.918       0.991: 100% 323/323 [00:42<00:00,  7.64it/s]\n",
            "    15/100    0.558G        1.03       0.828       0.962       0.998: 100% 323/323 [00:42<00:00,  7.65it/s]\n",
            "    16/100    0.558G        1.01       0.835       0.959       0.997: 100% 323/323 [00:42<00:00,  7.53it/s]\n",
            "    17/100    0.558G           1       0.813       0.969       0.998: 100% 323/323 [00:43<00:00,  7.39it/s]\n",
            "    18/100    0.558G       0.988       0.822       0.963           1: 100% 323/323 [00:42<00:00,  7.64it/s]\n",
            "    19/100    0.558G       0.981       0.826       0.967       0.997: 100% 323/323 [00:41<00:00,  7.76it/s]\n",
            "    20/100    0.558G       0.982       0.773       0.981           1: 100% 323/323 [00:43<00:00,  7.51it/s]\n",
            "    21/100    0.558G       0.967       0.796        0.98       0.998: 100% 323/323 [00:43<00:00,  7.43it/s]\n",
            "    22/100    0.558G       0.956       0.782       0.978           1: 100% 323/323 [00:42<00:00,  7.60it/s]\n",
            "    23/100    0.558G       0.955       0.778       0.984       0.997: 100% 323/323 [00:41<00:00,  7.85it/s]\n",
            "    24/100    0.558G       0.946       0.766       0.983           1: 100% 323/323 [00:43<00:00,  7.46it/s]\n",
            "    25/100    0.558G       0.947       0.768       0.979           1: 100% 323/323 [00:43<00:00,  7.37it/s]\n",
            "    26/100    0.558G       0.942       0.756       0.987           1: 100% 323/323 [00:42<00:00,  7.62it/s]\n",
            "    27/100    0.558G       0.946       0.748       0.992       0.999: 100% 323/323 [00:42<00:00,  7.59it/s]\n",
            "    28/100    0.558G       0.932       0.755       0.985           1: 100% 323/323 [00:43<00:00,  7.48it/s]\n",
            "    29/100    0.558G       0.931       0.755       0.985           1: 100% 323/323 [00:43<00:00,  7.35it/s]\n",
            "    30/100    0.558G       0.922        0.75       0.986           1: 100% 323/323 [00:41<00:00,  7.84it/s]\n",
            "    31/100    0.558G       0.923        0.75       0.985       0.999: 100% 323/323 [00:43<00:00,  7.41it/s]\n",
            "    32/100    0.558G        0.92       0.745       0.985           1: 100% 323/323 [00:43<00:00,  7.38it/s]\n",
            "    33/100    0.558G       0.918       0.753       0.985           1: 100% 323/323 [00:42<00:00,  7.66it/s]\n",
            "    34/100    0.558G       0.907       0.753       0.984           1: 100% 323/323 [00:41<00:00,  7.83it/s]\n",
            "    35/100    0.558G       0.901       0.744        0.99           1: 100% 323/323 [00:43<00:00,  7.45it/s]\n",
            "    36/100    0.558G       0.904       0.744       0.988           1: 100% 323/323 [00:43<00:00,  7.41it/s]\n",
            "    37/100    0.558G       0.902       0.739       0.989           1: 100% 323/323 [00:46<00:00,  7.00it/s]\n",
            "    38/100    0.558G       0.896       0.736       0.989           1: 100% 323/323 [00:43<00:00,  7.48it/s]\n",
            "    39/100    0.558G       0.896       0.736       0.989           1: 100% 323/323 [00:43<00:00,  7.37it/s]\n",
            "    40/100    0.558G       0.889       0.735       0.989           1: 100% 323/323 [00:42<00:00,  7.52it/s]\n",
            "    41/100    0.558G       0.894       0.732       0.991           1: 100% 323/323 [00:43<00:00,  7.44it/s]\n",
            "    42/100    0.558G       0.887        0.73        0.99           1: 100% 323/323 [00:41<00:00,  7.81it/s]\n",
            "    43/100    0.558G       0.884       0.732       0.991           1: 100% 323/323 [00:43<00:00,  7.36it/s]\n",
            "    44/100    0.558G       0.896        0.73       0.992           1: 100% 323/323 [00:42<00:00,  7.52it/s]\n",
            "    45/100    0.558G       0.874        0.73       0.992           1: 100% 323/323 [00:42<00:00,  7.60it/s]\n",
            "    46/100    0.558G       0.869       0.731        0.99           1: 100% 323/323 [00:49<00:00,  6.57it/s]\n",
            "    47/100    0.558G        0.87       0.731       0.989           1: 100% 323/323 [00:41<00:00,  7.75it/s]\n",
            "    48/100    0.558G       0.873       0.729        0.99           1: 100% 323/323 [00:45<00:00,  7.14it/s]\n",
            "    49/100    0.558G       0.867       0.728        0.99           1: 100% 323/323 [00:44<00:00,  7.30it/s]\n",
            "    50/100    0.558G       0.872       0.726       0.992           1: 100% 323/323 [00:44<00:00,  7.30it/s]\n",
            "    51/100    0.558G       0.865       0.724        0.99           1: 100% 323/323 [00:44<00:00,  7.20it/s]\n",
            "    52/100    0.558G        0.86       0.724        0.99           1: 100% 323/323 [00:41<00:00,  7.75it/s]\n",
            "    53/100    0.558G       0.863       0.725        0.99           1: 100% 323/323 [00:44<00:00,  7.30it/s]\n",
            "    54/100    0.558G       0.862       0.723       0.991           1: 100% 323/323 [00:43<00:00,  7.38it/s]\n",
            "    55/100    0.558G       0.856       0.722       0.993           1: 100% 323/323 [00:42<00:00,  7.67it/s]\n",
            "    56/100    0.558G       0.855       0.723       0.992           1: 100% 323/323 [00:43<00:00,  7.38it/s]\n",
            "    57/100    0.558G        0.85       0.722       0.993           1: 100% 323/323 [00:43<00:00,  7.42it/s]\n",
            "    58/100    0.558G       0.849       0.721       0.994           1: 100% 323/323 [00:42<00:00,  7.64it/s]\n",
            "    59/100    0.558G       0.844        0.72       0.994           1: 100% 323/323 [00:41<00:00,  7.69it/s]\n",
            "    60/100    0.558G       0.849        0.72       0.994           1: 100% 323/323 [00:43<00:00,  7.47it/s]\n",
            "    61/100    0.558G       0.837       0.719       0.994           1: 100% 323/323 [00:42<00:00,  7.52it/s]\n",
            "    62/100    0.558G       0.844       0.719       0.993           1: 100% 323/323 [00:41<00:00,  7.73it/s]\n",
            "    63/100    0.558G       0.841       0.719       0.993           1: 100% 323/323 [00:43<00:00,  7.41it/s]\n",
            "    64/100    0.558G       0.835       0.718       0.994           1: 100% 323/323 [00:43<00:00,  7.35it/s]\n",
            "    65/100    0.558G       0.834       0.718       0.994           1: 100% 323/323 [00:40<00:00,  7.96it/s]\n",
            "    66/100    0.558G        0.83       0.718       0.994           1: 100% 323/323 [00:44<00:00,  7.31it/s]\n",
            "    67/100    0.558G       0.833        0.72       0.994           1: 100% 323/323 [00:43<00:00,  7.39it/s]\n",
            "    68/100    0.558G        0.83       0.717       0.994           1: 100% 323/323 [00:42<00:00,  7.63it/s]\n",
            "    69/100    0.558G       0.823        0.72       0.994           1: 100% 323/323 [00:42<00:00,  7.51it/s]\n",
            "    70/100    0.558G       0.821       0.719       0.994           1: 100% 323/323 [00:43<00:00,  7.38it/s]\n",
            "    71/100    0.558G       0.823       0.717       0.994           1: 100% 323/323 [00:42<00:00,  7.54it/s]\n",
            "    72/100    0.558G       0.822       0.716       0.994           1: 100% 323/323 [00:41<00:00,  7.84it/s]\n",
            "    73/100    0.558G       0.819       0.716       0.994           1: 100% 323/323 [00:43<00:00,  7.45it/s]\n",
            "    74/100    0.558G       0.816       0.716       0.994           1: 100% 323/323 [00:43<00:00,  7.48it/s]\n",
            "    75/100    0.558G       0.816       0.716       0.994           1: 100% 323/323 [00:42<00:00,  7.58it/s]\n",
            "    76/100    0.558G       0.812       0.716       0.994           1: 100% 323/323 [00:42<00:00,  7.60it/s]\n",
            "    77/100    0.558G       0.813       0.716       0.994           1: 100% 323/323 [00:44<00:00,  7.34it/s]\n",
            "    78/100    0.558G       0.808       0.716       0.994           1: 100% 323/323 [00:43<00:00,  7.49it/s]\n",
            "    79/100    0.558G       0.808       0.714       0.994           1: 100% 323/323 [00:41<00:00,  7.84it/s]\n",
            "    80/100    0.558G       0.804       0.714       0.994           1: 100% 323/323 [00:42<00:00,  7.51it/s]\n",
            "    81/100    0.558G        0.81       0.714       0.994           1: 100% 323/323 [00:43<00:00,  7.38it/s]\n",
            "    82/100    0.558G       0.799       0.714       0.994           1: 100% 323/323 [00:41<00:00,  7.77it/s]\n",
            "    83/100    0.558G       0.798       0.714       0.994           1: 100% 323/323 [00:43<00:00,  7.37it/s]\n",
            "    84/100    0.558G       0.801       0.713       0.994           1: 100% 323/323 [00:44<00:00,  7.33it/s]\n",
            "    85/100    0.558G       0.792       0.713       0.994           1: 100% 323/323 [00:41<00:00,  7.72it/s]\n",
            "    86/100    0.558G       0.791       0.713       0.993           1: 100% 323/323 [00:41<00:00,  7.75it/s]\n",
            "    87/100    0.558G       0.798       0.713       0.993           1: 100% 323/323 [00:42<00:00,  7.56it/s]\n",
            "    88/100    0.558G       0.789       0.714       0.993           1: 100% 323/323 [00:43<00:00,  7.38it/s]\n",
            "    89/100    0.558G       0.787       0.713       0.993           1: 100% 323/323 [00:41<00:00,  7.83it/s]\n",
            "    90/100    0.558G       0.787       0.713       0.993           1: 100% 323/323 [00:43<00:00,  7.43it/s]\n",
            "    91/100    0.558G       0.788       0.713       0.993           1: 100% 323/323 [00:43<00:00,  7.34it/s]\n",
            "    92/100    0.558G       0.786       0.713       0.993           1: 100% 323/323 [00:42<00:00,  7.64it/s]\n",
            "    93/100    0.558G       0.778       0.712       0.993           1: 100% 323/323 [00:41<00:00,  7.69it/s]\n",
            "    94/100    0.558G       0.781       0.713       0.993           1: 100% 323/323 [00:43<00:00,  7.44it/s]\n",
            "    95/100    0.558G       0.781       0.712       0.993           1: 100% 323/323 [00:42<00:00,  7.53it/s]\n",
            "    96/100    0.558G       0.775       0.712       0.993           1: 100% 323/323 [00:41<00:00,  7.81it/s]\n",
            "    97/100    0.558G       0.777       0.712       0.992           1: 100% 323/323 [00:43<00:00,  7.39it/s]\n",
            "    98/100    0.558G       0.771       0.714       0.992           1: 100% 323/323 [00:43<00:00,  7.45it/s]\n",
            "    99/100    0.558G       0.772       0.713       0.992           1: 100% 323/323 [00:41<00:00,  7.81it/s]\n",
            "   100/100    0.558G       0.773       0.714       0.992           1: 100% 323/323 [00:42<00:00,  7.61it/s]\n",
            "\n",
            "Training complete (1.198 hours)\n",
            "Results saved to \u001b[1mruns/train-cls/exp\u001b[0m\n",
            "Predict:         python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source im.jpg\n",
            "Validate:        python classify/val.py --weights runs/train-cls/exp/weights/best.pt --data /content/datasets/Traffic-Signs-Detection-1\n",
            "Export:          python export.py --weights runs/train-cls/exp/weights/best.pt --include onnx\n",
            "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp/weights/best.pt')\n",
            "Visualize:       https://netron.app\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate Your Custom Model\n",
        "\n",
        "Repeat step 2 from above to test and validate your custom model."
      ],
      "metadata": {
        "id": "HHUFGeLbGd98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python classify/val.py --weights runs/train-cls/exp/weights/best.pt --data ../datasets/$DATASET_NAME"
      ],
      "metadata": {
        "id": "DIV7ydyKGZFL",
        "outputId": "46982389-ad88-427f-e7e5-046aceee10af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mclassify/val: \u001b[0mdata=../datasets/Traffic-Signs-Detection-1, weights=['runs/train-cls/exp/weights/best.pt'], batch_size=128, imgsz=224, device=, workers=8, verbose=True, project=runs/val-cls, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 ğŸš€ v7.0-377-g24ee2801 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 117 layers, 4221771 parameters, 0 gradients, 10.4 GFLOPs\n",
            "testing:   0% 0/8 [00:00<?, ?it/s]/content/yolov5/classify/val.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=device.type != \"cpu\"):\n",
            "testing: 100% 8/8 [00:04<00:00,  1.89it/s]\n",
            "                   Class      Images    top1_acc    top5_acc\n",
            "                     all         983       0.995           1\n",
            "              Ahead only          25           1           1\n",
            "      Beware of ice-snow          20           1           1\n",
            "       Bicycles crossing          22           1           1\n",
            "              Bumpy road          20           1           1\n",
            "       Children crossing          27           1           1\n",
            "Dangerous curve to the left          23           1           1\n",
            "Dangerous curve to the right          23           1           1\n",
            "            Double curve          20           1           1\n",
            "End of all speed and passing limits          17           1           1\n",
            "       End of no passing          27           1           1\n",
            "End of no passing by vehicles over 3-5 metric tons          22           1           1\n",
            "End of speed limit (80km-h)          20           1           1\n",
            "         General caution          24           1           1\n",
            "     Go straight or left          25           1           1\n",
            "    Go straight or right          28           1           1\n",
            "               Keep left          28       0.929           1\n",
            "              Keep right          28           1           1\n",
            "                No entry          21           1           1\n",
            "              No passing          27           1           1\n",
            "No passing for vehicles over 3-5 metric tons          23           1           1\n",
            "             No vehicles          26           1           1\n",
            "             Pedestrians          26           1           1\n",
            "           Priority road          21           1           1\n",
            "Right-of-way at the next intersection          19           1           1\n",
            "Road narrows on the right          19           1           1\n",
            "               Road work          24           1           1\n",
            "    Roundabout mandatory          21           1           1\n",
            "           Slippery road          21           1           1\n",
            "   Speed limit (100km-h)          25           1           1\n",
            "   Speed limit (120km-h)          32           1           1\n",
            "    Speed limit (20km-h)          17           1           1\n",
            "    Speed limit (30km-h)          20           1           1\n",
            "    Speed limit (50km-h)          18           1           1\n",
            "    Speed limit (60km-h)          27       0.926           1\n",
            "    Speed limit (70km-h)          21           1           1\n",
            "    Speed limit (80km-h)          30           1           1\n",
            "                    Stop          22           1           1\n",
            "         Traffic signals          18           1           1\n",
            "         Turn left ahead          18           1           1\n",
            "        Turn right ahead          24       0.958           1\n",
            "Vehicles over 3-5 metric tons prohibited          21           1           1\n",
            "   Wild animals crossing          25           1           1\n",
            "                   Yield          18           1           1\n",
            "Speed: 0.1ms pre-process, 1.9ms inference, 0.1ms post-process per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/val-cls/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Infer With Your Custom Model"
      ],
      "metadata": {
        "id": "uH5tJNpEsi6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the path of an image from the test or validation set\n",
        "if os.path.exists(os.path.join(dataset.location, \"test\")):\n",
        "  split_path = os.path.join(dataset.location, \"test\")\n",
        "else:\n",
        "  os.path.join(dataset.location, \"valid\")\n",
        "example_class = os.listdir(split_path)[0]\n",
        "example_image_name = os.listdir(os.path.join(split_path, example_class))[0]\n",
        "example_image_path = os.path.join(split_path, example_class, example_image_name)\n",
        "os.environ[\"/content/e10f94_096caabe7135431bbe50f781d504626c~mv2.jpg\"] = example_image_path\n",
        "\n",
        "print(f\"Inferring on an example of the class '{example_class}'\")\n",
        "\n",
        "#Infer\n",
        "!python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source $TEST_IMAGE_PATH"
      ],
      "metadata": {
        "id": "81lK1hU_sk54",
        "outputId": "c3828a40-0076-4a8d-dc43-531324e59fe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inferring on an example of the class 'End of no passing by vehicles over 3-5 metric tons'\n",
            "usage: predict.py [-h] [--weights WEIGHTS [WEIGHTS ...]] [--source SOURCE] [--data DATA]\n",
            "                  [--imgsz IMGSZ [IMGSZ ...]] [--device DEVICE] [--view-img] [--save-txt]\n",
            "                  [--nosave] [--augment] [--visualize] [--update] [--project PROJECT]\n",
            "                  [--name NAME] [--exist-ok] [--half] [--dnn] [--vid-stride VID_STRIDE]\n",
            "predict.py: error: unrecognized arguments: of no passing by vehicles over 3-5 metric tons/00042_00002_00005_png.rf.e108b1da0bfa0508423735410a62c80b.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the inference results show ~3ms inference and the respective classes predicted probabilities."
      ],
      "metadata": {
        "id": "DdGuG-1kNjWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (OPTIONAL) Improve Our Model with Active Learning\n",
        "\n",
        "Now that we've trained our model once, we will want to continue to improve its performance. Improvement is largely dependent on improving our dataset.\n",
        "\n",
        "We can programmatically upload example failure images back to our custom dataset based on conditions (like seeing an underrpresented class or a low confidence score) using the same `pip` package."
      ],
      "metadata": {
        "id": "I38IM6NXKNN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Upload example image\n",
        "# project.upload(image_path)\n"
      ],
      "metadata": {
        "id": "HycgSEnYKo0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example upload code\n",
        "# min_conf = float(\"inf\")\n",
        "# for pred in results:\n",
        "#     if pred[\"score\"] < min_conf:\n",
        "#         min_conf = pred[\"score\"]\n",
        "# if min_conf < 0.4:\n",
        "#     project.upload(image_path)"
      ],
      "metadata": {
        "id": "VwXDoz_vLK3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (BONUS) YOLOv5 classify/predict.py Accepts Several Input Methods\n",
        "- Webcam: `python classify/predict.py --weights yolov5s-cls.pt --source 0`\n",
        "- Image `python classify/predict.py --weights yolov5s-cls.pt --source img.jpg`\n",
        "- Video: `python classify/predict.py --weights yolov5s-cls.pt --source vid.mp4`\n",
        "- Directory: `python classify/predict.py --weights yolov5s-cls.pt --source path/`\n",
        "- Glob: `python classify/predict.py --weights yolov5s-cls.pt --source 'path/*.jpg'`\n",
        "- YouTube: `python classify/predict.py --weights yolov5s-cls.pt --source 'https://youtu.be/Zgi9g1ksQHc'`\n",
        "- RTSP, RTMP, HTTP stream: `python classify/predict.py --weights yolov5s-cls.pt --source 'rtsp://example.com/media.mp4'`"
      ],
      "metadata": {
        "id": "aYlfaHDusN-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Directory Example"
      ],
      "metadata": {
        "id": "iKSP-SNTvcLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Directory infer\n",
        "# os.environ[\"TEST_CLASS_PATH\"] = test_class_path = os.path.join(*os.environ[\"TEST_IMAGE_PATH\"].split(os.sep)[:-1])\n",
        "# print(f\"Infering on all images from the directory {os.environ['TEST_CLASS_PATH']}\")\n",
        "# !python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source /$TEST_CLASS_PATH/"
      ],
      "metadata": {
        "id": "lwSoHcHcvjeD",
        "outputId": "1fca5109-6edf-468c-89bb-600e3dd9b393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infering on all images from the directory content/datasets/Traffic-Signs-Detection-1/test/End of no passing by vehicles over 3-5 metric tons\n",
            "usage: predict.py [-h] [--weights WEIGHTS [WEIGHTS ...]] [--source SOURCE] [--data DATA]\n",
            "                  [--imgsz IMGSZ [IMGSZ ...]] [--device DEVICE] [--view-img] [--save-txt]\n",
            "                  [--nosave] [--augment] [--visualize] [--update] [--project PROJECT]\n",
            "                  [--name NAME] [--exist-ok] [--half] [--dnn] [--vid-stride VID_STRIDE]\n",
            "predict.py: error: unrecognized arguments: of no passing by vehicles over 3-5 metric tons/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###YouTube Example"
      ],
      "metadata": {
        "id": "kCCao9t8se8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YouTube infer\n",
        "!python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source 'https://www.youtube.com/watch?v=oW6m7fABIGs&pp=ygUNdHJhZmZpYyBzaWducw%3D%3D'"
      ],
      "metadata": {
        "id": "heebjpJBsakV",
        "outputId": "0d63d739-e13a-477a-871f-3d17706cd676",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=https://www.youtube.com/watch?v=7AlYA4ItA74, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=False, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 ğŸš€ v7.0-377-g24ee2801 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 117 layers, 4221771 parameters, 0 gradients, 10.4 GFLOPs\n",
            "WARNING âš ï¸ Environment does not support cv2.imshow() or PIL Image.show()\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['pafy', 'youtube_dl==2020.12.2'] not found, attempting AutoUpdate...\n",
            "Collecting pafy\n",
            "  Downloading pafy-0.5.5-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting youtube_dl==2020.12.2\n",
            "  Downloading youtube_dl-2020.12.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading youtube_dl-2020.12.2-py2.py3-none-any.whl (1.8 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.8/1.8 MB 45.5 MB/s eta 0:00:00\n",
            "Downloading pafy-0.5.5-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: youtube_dl, pafy\n",
            "Successfully installed pafy-0.5.5 youtube_dl-2020.12.2\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 5.2s, installed 2 packages: ['pafy', 'youtube_dl==2020.12.2']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "[ WARN:0@9.791] global cap.cpp:164 open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
            "\n",
            "OpenCV(4.10.0) /io/opencv/modules/videoio/src/cap_images.cpp:244: error: (-5:Bad argument) CAP_IMAGES: error, expected '0?[1-9][du]' pattern, got: https://rr3---sn-q4fl6nds.googlevideo.com/videoplayback?expire=1729482767&ei=r3sVZ7GGAqmpsfIPoqua8Ao&ip=34.125.228.122&id=o-ACTDwDoXcywm8eWx0pRnBQUplLIgcqQPY51b3x2gAZP6&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1729461167%2C&mh=4p&mm=31%2C29&mn=sn-q4fl6nds%2Csn-q4flrnl6&ms=au%2Crdu&mv=m&mvi=3&pl=20&rms=au%2Cau&initcwndbps=8297500&bui=AXLXGFQgng0ZrIPZK7p3glUA1fsxXp5prDdqsIVT40lcNTKJclFWMf9s6FrpUKP3CR6YCbJEm-YJu_PC&spc=54MbxRMapW_IH2dI4Pucla8c_SoVlIy-JVQOs421rsJVN65YNMK7NyFDCI4Mpo4&vprv=1&svpuc=1&mime=video%2Fmp4&ns=3seuajh_Q3Qbrlmftufy57YQ&rqh=1&cnr=14&ratebypass=yes&dur=297.099&lmt=1711440897653992&mt=1729460713&fvip=4&fexp=51312688%2C51326932&c=WEB&sefc=1&txp=8218224&n=Ty3iua_guIDwYCK&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Ccnr%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRQIhANMv3jAESWAfZGj5cg5b5vC5CRAk7ZeswXJQ9jflbEVUAiBelKtu4KaHQKNTWaE_N6V_PWYCfcxn1kKkBK2CSMHT2g%3D%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=ACJ0pHgwRQIhAI9UEKeNk8DITWIt4nZt6qXFNoa6Nqe5HWO78GTKWG_sAiBg2vKdaqe9poxh_8muewLWMQH0F9KV2ccxrlk-MTCgLg%3D%3D in function 'icvExtractPattern'\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/classify/predict.py\", line 241, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/classify/predict.py\", line 236, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/yolov5/classify/predict.py\", line 111, in run\n",
            "    dataset = LoadStreams(source, img_size=imgsz, transforms=classify_transforms(imgsz[0]), vid_stride=vid_stride)\n",
            "  File \"/content/yolov5/utils/dataloaders.py\", line 464, in __init__\n",
            "    assert cap.isOpened(), f\"{st}Failed to open {s}\"\n",
            "AssertionError: 1/1: https://www.youtube.com/watch?v=7AlYA4ItA74... Failed to open https://rr3---sn-q4fl6nds.googlevideo.com/videoplayback?expire=1729482767&ei=r3sVZ7GGAqmpsfIPoqua8Ao&ip=34.125.228.122&id=o-ACTDwDoXcywm8eWx0pRnBQUplLIgcqQPY51b3x2gAZP6&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1729461167%2C&mh=4p&mm=31%2C29&mn=sn-q4fl6nds%2Csn-q4flrnl6&ms=au%2Crdu&mv=m&mvi=3&pl=20&rms=au%2Cau&initcwndbps=8297500&bui=AXLXGFQgng0ZrIPZK7p3glUA1fsxXp5prDdqsIVT40lcNTKJclFWMf9s6FrpUKP3CR6YCbJEm-YJu_PC&spc=54MbxRMapW_IH2dI4Pucla8c_SoVlIy-JVQOs421rsJVN65YNMK7NyFDCI4Mpo4&vprv=1&svpuc=1&mime=video%2Fmp4&ns=3seuajh_Q3Qbrlmftufy57YQ&rqh=1&cnr=14&ratebypass=yes&dur=297.099&lmt=1711440897653992&mt=1729460713&fvip=4&fexp=51312688%2C51326932&c=WEB&sefc=1&txp=8218224&n=Ty3iua_guIDwYCK&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Ccnr%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRQIhANMv3jAESWAfZGj5cg5b5vC5CRAk7ZeswXJQ9jflbEVUAiBelKtu4KaHQKNTWaE_N6V_PWYCfcxn1kKkBK2CSMHT2g%3D%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=ACJ0pHgwRQIhAI9UEKeNk8DITWIt4nZt6qXFNoa6Nqe5HWO78GTKWG_sAiBg2vKdaqe9poxh_8muewLWMQH0F9KV2ccxrlk-MTCgLg%3D%3D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testando o modelo**"
      ],
      "metadata": {
        "id": "vEqK7YLJapBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "V6PqrpiYarGf"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminhos para o modelo e a imagem (ajuste conforme necessÃ¡rio)\n",
        "model_path = '/content/best.pt'  # Substitua pelo caminho do seu modelo\n",
        "img_path = '/content/stop-sign.jpg'      # Stop sign\n",
        "# img_path = '/content/general-caution-sign.jpg'    # General caution sign\n",
        "# img_path = '/content/speed-limit-30kmh.jpg'        # Speed limit 30 km/h"
      ],
      "metadata": {
        "id": "TvVoFguXaxMG"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Carregar o modelo\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)\n",
        "\n",
        "# Se a GPU estiver disponÃ­vel, mover o modelo para GPU\n",
        "if torch.cuda.is_available():\n",
        "    model.to('cuda')\n",
        "\n",
        "# Carregar a imagem\n",
        "img = Image.open(img_path)\n",
        "\n",
        "# Converter a imagem PIL para um array NumPy\n",
        "img = np.array(img)\n",
        "\n",
        "# Redimensionar a imagem para o tamanho esperado pelo modelo (ex.: 224x224)\n",
        "img = cv2.resize(img, (224, 224))\n",
        "\n",
        "# Converter o array NumPy para um tensor PyTorch e mudar o tipo de dado para float32\n",
        "img = torch.from_numpy(img).type(torch.float32)\n",
        "\n",
        "# Normalizar a imagem (dividir os valores de pixel por 255)\n",
        "img /= 255.0\n",
        "\n",
        "# Adicionar a dimensÃ£o do lote (batch)\n",
        "img = img.permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "# Mover a imagem para o mesmo dispositivo do modelo (GPU, se disponÃ­vel)\n",
        "if torch.cuda.is_available():\n",
        "    img = img.to('cuda')\n",
        "\n",
        "# Lista de classes de sinais de trÃ¢nsito\n",
        "classes = [\n",
        "    'Ahead only', 'Beware of ice-snow', 'Bicycles crossing', 'Bumpy road',\n",
        "    'Children crossing', 'Dangerous curve to the left', 'Dangerous curve to the right', 'Double curve',\n",
        "    'End of all speed and passing limits', 'End of no passing', 'End of no passing by vehicles over 3-5 metric tons',\n",
        "    'End of speed limit (80km-h)', 'General caution', 'Go straight or left', 'Go straight or right',\n",
        "    'Keep left', 'Keep right', 'No entry', 'No passing', 'No passing for vehicles over 3-5 metric tons',\n",
        "    'No vehicles', 'Pedestrians', 'Priority road', 'Right-of-way at the next intersection', 'Road narrows on the right',\n",
        "    'Road work', 'Roundabout mandatory', 'Slippery road', 'Speed limit (100km-h)', 'Speed limit (120km-h)',\n",
        "    'Speed limit (20km-h)', 'Speed limit (30km-h)', 'Speed limit (50km-h)', 'Speed limit (60km-h)',\n",
        "    'Speed limit (70km-h)', 'Speed limit (80km-h)', 'Stop', 'Traffic signals', 'Turn left ahead', 'Turn right ahead',\n",
        "    'Vehicles over 3-5 metric tons prohibited', 'Wild animals crossing', 'Yield'\n",
        "]\n",
        "\n",
        "# Realizar a classificaÃ§Ã£o\n",
        "results = model(img)\n",
        "\n",
        "# Aplicar softmax para obter as probabilidades das classes\n",
        "probabilities = F.softmax(results, dim=1)\n",
        "\n",
        "# Obter o Ã­ndice da classe prevista\n",
        "predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "# Obter a confianÃ§a (probabilidade) da classe prevista\n",
        "confidence = probabilities[0][predicted_class_idx].item()\n",
        "\n",
        "# Exibir o nome da classe prevista e a confianÃ§a\n",
        "predicted_class_name = classes[predicted_class_idx]\n",
        "print(f'PrediÃ§Ã£o de classe: {predicted_class_idx} - {predicted_class_name}')\n",
        "print(f'ConfianÃ§a do modelo: {confidence * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If_EpZr56jJk",
        "outputId": "7bf66809-429c-40a9-e1e2-4efa5dc55ecd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ğŸš€ v7.0-377-g24ee2801 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 117 layers, 4221771 parameters, 0 gradients, 10.4 GFLOPs\n",
            "WARNING âš ï¸ YOLOv5 ClassificationModel is not yet AutoShape compatible. You must pass torch tensors in BCHW to this model, i.e. shape(1,3,224,224).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PrediÃ§Ã£o de classe: 12 - General caution\n",
            "ConfianÃ§a do modelo: 28.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Lista de classes de sinais de trÃ¢nsito\n",
        "classes = [\n",
        "    'Ahead only', 'Beware of ice-snow', 'Bicycles crossing', 'Bumpy road',\n",
        "    'Children crossing', 'Dangerous curve to the left', 'Dangerous curve to the right', 'Double curve',\n",
        "    'End of all speed and passing limits', 'End of no passing', 'End of no passing by vehicles over 3-5 metric tons',\n",
        "    'End of speed limit (80km-h)', 'General caution', 'Go straight or left', 'Go straight or right',\n",
        "    'Keep left', 'Keep right', 'No entry', 'No passing', 'No passing for vehicles over 3-5 metric tons',\n",
        "    'No vehicles', 'Pedestrians', 'Priority road', 'Right-of-way at the next intersection', 'Road narrows on the right',\n",
        "    'Road work', 'Roundabout mandatory', 'Slippery road', 'Speed limit (100km-h)', 'Speed limit (120km-h)',\n",
        "    'Speed limit (20km-h)', 'Speed limit (30km-h)', 'Speed limit (50km-h)', 'Speed limit (60km-h)',\n",
        "    'Speed limit (70km-h)', 'Speed limit (80km-h)', 'Stop', 'Traffic signals', 'Turn left ahead', 'Turn right ahead',\n",
        "    'Vehicles over 3-5 metric tons prohibited', 'Wild animals crossing', 'Yield'\n",
        "]\n",
        "\n",
        "# Realizar a classificaÃ§Ã£o\n",
        "results = model(img)\n",
        "\n",
        "# Aplicar softmax para converter logits em probabilidades\n",
        "probabilities = F.softmax(results, dim=1)\n",
        "\n",
        "# Obter o Ã­ndice da classe prevista\n",
        "predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "# Obter a confianÃ§a da classe prevista\n",
        "confidence = probabilities[0][predicted_class_idx].item()\n",
        "\n",
        "# Exibir o nome da classe correspondente e a confianÃ§a\n",
        "predicted_class_name = classes[predicted_class_idx]\n",
        "print(f'PrediÃ§Ã£o de classe: {predicted_class_idx} - {predicted_class_name}')\n",
        "print(f'ConfianÃ§a do modelo: {confidence * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWsCph2s592E",
        "outputId": "4a1e556f-3e82-4f5a-ffc0-41103f64bfdb"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PrediÃ§Ã£o de classe: 12 - General caution\n",
            "ConfianÃ§a do modelo: 28.53%\n"
          ]
        }
      ]
    }
  ]
}