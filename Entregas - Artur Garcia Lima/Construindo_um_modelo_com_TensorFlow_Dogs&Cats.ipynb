{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu5OLDcwuSoe"
      },
      "source": [
        "# **Construindo um Modelo com Tensorflow -  ğŸ¶ ğŸ±!**\n",
        "\n",
        "**Problema: uma imagem, dizer se Ã© um dog ou um cat.**\n",
        "\n",
        "Computer Vision; Deep Learning; Machine Learning; Artificial Inteligence! Nada disso faz sentido sem dados, muitos dados (Big Data! ğŸš€). Para isso, teremos:\n",
        "\n",
        "* **Treinamento**: 25.000 imagens nomeadas: 12.500 de dogs e 12.500 de cats.\n",
        "* **Teste**: 1.000 imagens de dogs e cats.\n",
        "\n",
        "Usa-se os dados de treino para treinar o algoritmo e entÃ£o criar o modelo preditivo. Usa-se os dados de teste para confirmar o desempenho do modelo preditivo jÃ¡ treinado, ou seja, apresenta-se ao modelo preditivo dados que ele nÃ£o viu durante o treinamento, a fim de garantir que ele seja capaz de fazer previsÃµes.\n",
        "\n",
        "Por fim, o modelo de duas camadas de convoluÃ§Ã£o seguidas de pooling, a camada de flattening, e as camadas totalmente conectadas (Dense), com a funÃ§Ã£o de ativaÃ§Ã£o sigmoid para a saÃ­da binÃ¡ria.\n",
        "\n",
        "**NÃ£o hÃ¡ mÃ¡gica. HÃ¡ matemÃ¡tica!** ğŸ§™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv1N0nRuxwL-"
      },
      "source": [
        "**Fonte de dados**\n",
        "\n",
        "O Kaggle oferece diversos datasets pÃºblicos que podem ser usados para vocÃª desenvolver seus projetos e incluir no seu portfÃ³lio, uma excelente forma de demonstrar suas habilidades em Data Science e Machine Learning. Usaremos como fonte de dados, o famoso [dataset Dogs and Cats](https://www.kaggle.com/c/dogs-vs-cats/data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B80jFLJAym_v"
      },
      "source": [
        "**Carregando os dados de Treino e Teste**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "wD9uqxNTtumI",
        "outputId": "7c2fdf76-c40a-49f1-af61-ffa2ef7a2ec1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN2T6cv24Y6A"
      },
      "source": [
        "**Construindo a Rede Neural Convolucional**\n",
        "\n",
        "O Keras Ã© uma biblioteca do TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "LNpCfaa8NfdY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "eb0TUqkQNjaI"
      },
      "outputs": [],
      "source": [
        "import keras as K\n",
        "import os\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "XJldwmiJ4hHA"
      },
      "outputs": [],
      "source": [
        "# Importar K e suas funÃ§Ãµes necessÃ¡rias\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from keras.layers import Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "MYZrHMd85PqQ"
      },
      "outputs": [],
      "source": [
        "# Inicializando a Rede Neural Convolucional\n",
        "classifier = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrympYJ96BWg",
        "outputId": "8e084778-0b18-455f-da66-29d9493124e5"
      },
      "outputs": [],
      "source": [
        "# 32 features; formato 3x3; Imagens 64x64; Array 3D (RGB).\n",
        "\n",
        "# Adicionando a Primeira Camada de ConvoluÃ§Ã£o\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape = (256, 256, 3), activation = 'relu'))\n",
        "\n",
        "# Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adicionando a Segunda Camada de ConvoluÃ§Ã£o\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "\n",
        "# Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adicionando a TERCEIRA Camada de ConvoluÃ§Ã£o\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "\n",
        "# Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Flattening - Transformando em 1D\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Full connection\n",
        "classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "# Compilando a rede\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1l-VgZ78NNS"
      },
      "source": [
        "**PrÃ©-processamento**\n",
        "\n",
        "Fazer prÃ©-processamento nos dados, em nosso caso as imagens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "T8au9i8V7mDZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Criar o objeto com as regras de prÃ©-processamento\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=30,  # Adiciona rotaÃ§Ã£o\n",
        "    width_shift_range=0.1,  # Deslocamento horizontal\n",
        "    height_shift_range=0.1,  # Deslocamento vertical\n",
        "    fill_mode='nearest'  # Preencher pixels ausentes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "BLiOSL6h7vAI",
        "outputId": "59f5dad6-a86b-42fc-cdc5-d8bbdcd7fdee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 14002 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# PrÃ©-processamento das imagens de treino\n",
        "training_set = train_datagen.flow_from_directory('Dados/dogs-vs-cats/train',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 64,\n",
        "                                                 class_mode = 'binary',color_mode='rgb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ephZE1Y59LKq"
      },
      "source": [
        "**Treinamento do Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KVRGudo9No6",
        "outputId": "6384c6b0-068e-48d8-c874-aad97856cd39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - accuracy: 0.5046 - loss: 0.8012\n",
            "Epoch 2/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5327 - loss: 0.7039\n",
            "Epoch 3/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5207 - loss: 0.6924\n",
            "Epoch 4/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5703 - loss: 0.6862\n",
            "Epoch 5/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.4685 - loss: 0.6995\n",
            "Epoch 6/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.4826 - loss: 0.6961\n",
            "Epoch 7/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5151 - loss: 0.6962\n",
            "Epoch 8/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5344 - loss: 0.6896\n",
            "Epoch 9/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5412 - loss: 0.6910\n",
            "Epoch 10/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5592 - loss: 0.6855\n",
            "Epoch 11/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5181 - loss: 0.6890\n",
            "Epoch 12/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.6091 - loss: 0.6803\n",
            "Epoch 13/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5944 - loss: 0.6738\n",
            "Epoch 14/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5842 - loss: 0.6821\n",
            "Epoch 15/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5506 - loss: 0.6830\n",
            "Epoch 16/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5621 - loss: 0.6757\n",
            "Epoch 17/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5734 - loss: 0.6809\n",
            "Epoch 18/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5757 - loss: 0.6692\n",
            "Epoch 19/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5469 - loss: 0.6718\n",
            "Epoch 20/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5737 - loss: 0.6694\n",
            "Epoch 21/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5893 - loss: 0.6621\n",
            "Epoch 22/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5994 - loss: 0.6670\n",
            "Epoch 23/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.6013 - loss: 0.6513\n",
            "Epoch 24/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.7250 - loss: 0.6422\n",
            "Epoch 25/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5615 - loss: 0.6790\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x23b58fc9790>"
            ]
          },
          "execution_count": 256,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Executando o treinamento\n",
        "classifier.fit(training_set,\n",
        "               steps_per_epoch=5,\n",
        "               epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl1MFhzd9aph"
      },
      "source": [
        "Treinamento concluÃ­do com sucesso! ğŸ’ª Observe se ao final de cada Ã©poca a acurÃ¡cia aumenta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OIAQ0GU-WrK"
      },
      "source": [
        "**Teste do Modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQYol5tb-ejq"
      },
      "source": [
        "Testar o modelo treinado com imagens que ele ainda nÃ£o viu. Por fim, verificamos o resultado da previsÃ£o e emitimos a informaÃ§Ã£o se a imagem Ã© de um gato ou cachorro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "xthzuIX_IkU_",
        "outputId": "194a933a-4b6c-4fa6-a06a-c7486953e6c7"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_52\" is incompatible with the layer: expected axis -1 of input shape to have value 6272, but received input with shape (1, 123008)\u001b[0m\n\nArguments received by Sequential.call():\n  â€¢ inputs=tf.Tensor(shape=(1, 256, 256, 3), dtype=float32)\n  â€¢ training=False\n  â€¢ mask=None",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[257], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m test_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(test_image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Fazendo a previsÃ£o usando o modelo classifier\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m result \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(test_image)\n\u001b[0;32m     12\u001b[0m class_indices \u001b[38;5;241m=\u001b[39m training_set\u001b[38;5;241m.\u001b[39mclass_indices\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\art-g\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\art-g\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_52\" is incompatible with the layer: expected axis -1 of input shape to have value 6272, but received input with shape (1, 123008)\u001b[0m\n\nArguments received by Sequential.call():\n  â€¢ inputs=tf.Tensor(shape=(1, 256, 256, 3), dtype=float32)\n  â€¢ training=False\n  â€¢ mask=None"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from IPython.display import Image\n",
        "\n",
        "# Carregando a imagem de teste\n",
        "test_image = image.load_img('Dados/test/174.jpg', target_size=(256, 256))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "# Fazendo a previsÃ£o usando o modelo classifier\n",
        "result = classifier.predict(test_image)\n",
        "class_indices = training_set.class_indices\n",
        "\n",
        "if result[0][0] > 0.5:\n",
        "    prediction = 'Cachorro.'\n",
        "    accuracy = round(result[0][0] * 100, 2)\n",
        "else:\n",
        "    prediction = 'Gato.'\n",
        "    accuracy = round((1 - result[0][0]) * 100, 2)\n",
        "\n",
        "\n",
        "# Exibindo a previsÃ£o e a acurÃ¡cia\n",
        "print(\"PrevisÃ£o:\", prediction)\n",
        "print(\"AcurÃ¡cia:\", accuracy, \"%.\")\n",
        "\n",
        "\n",
        "Image(filename='Dados/test/174.jpg')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goRr2pHz-qrp"
      },
      "source": [
        "**O modelo recebeu uma imagem que nunca tinha visto antes e com base no que aprendeu durante o treinamento, foi capaz de classificar.**\n",
        "\n",
        "Convertemos a imagem de teste em um vetor de pixels e apresentamos ao modelo.\n",
        "O modelo compara o vetor da imagem de teste com seus pesos e entÃ£o emite a classificaÃ§Ã£o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfsCG3KP9kwr"
      },
      "source": [
        "**Melhorias adicionais para este modelo:**\n",
        "\n",
        "*   Aumentar o nÃºmero de Ã©pocas para 25 para uma aprendizagem mais profunda.\n",
        "*   aumentar o redimensionamento da imagem de 64x64 para 256x256.\n",
        "*   Aumentar o tamanho do lote de 32 para 64.\n",
        "*   Alterar a arquitetura da rede incluindo mais uma camada convolucional.\n",
        "*   Avaliar outras mÃ©tricas do modelo e ajustar os hiperparÃ¢metros de acordo.\n",
        "*   Experimentar outros algoritmos de otimizaÃ§Ã£o.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWz1StcE-MZz"
      },
      "source": [
        "Fim! ğŸ”¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
