{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu5OLDcwuSoe"
      },
      "source": [
        "# **Construindo um Modelo com Tensorflow -  ğŸ¶ ğŸ±!**\n",
        "\n",
        "**Problema: uma imagem, dizer se Ã© um dog ou um cat.**\n",
        "\n",
        "Computer Vision; Deep Learning; Machine Learning; Artificial Inteligence! Nada disso faz sentido sem dados, muitos dados (Big Data! ğŸš€). Para isso, teremos:\n",
        "\n",
        "* **Treinamento**: 25.000 imagens nomeadas: 12.500 de dogs e 12.500 de cats.\n",
        "* **Teste**: 1.000 imagens de dogs e cats.\n",
        "\n",
        "Usa-se os dados de treino para treinar o algoritmo e entÃ£o criar o modelo preditivo. Usa-se os dados de teste para confirmar o desempenho do modelo preditivo jÃ¡ treinado, ou seja, apresenta-se ao modelo preditivo dados que ele nÃ£o viu durante o treinamento, a fim de garantir que ele seja capaz de fazer previsÃµes.\n",
        "\n",
        "Por fim, o modelo de duas camadas de convoluÃ§Ã£o seguidas de pooling, a camada de flattening, e as camadas totalmente conectadas (Dense), com a funÃ§Ã£o de ativaÃ§Ã£o sigmoid para a saÃ­da binÃ¡ria.\n",
        "\n",
        "**NÃ£o hÃ¡ mÃ¡gica. HÃ¡ matemÃ¡tica!** ğŸ§™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv1N0nRuxwL-"
      },
      "source": [
        "**Fonte de dados**\n",
        "\n",
        "O Kaggle oferece diversos datasets pÃºblicos que podem ser usados para vocÃª desenvolver seus projetos e incluir no seu portfÃ³lio, uma excelente forma de demonstrar suas habilidades em Data Science e Machine Learning. Usaremos como fonte de dados, o famoso [dataset Dogs and Cats](https://www.kaggle.com/c/dogs-vs-cats/data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B80jFLJAym_v"
      },
      "source": [
        "**Carregando os dados de Treino e Teste**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wD9uqxNTtumI"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN2T6cv24Y6A"
      },
      "source": [
        "**Construindo a Rede Neural Convolucional**\n",
        "\n",
        "O Keras Ã© uma biblioteca do TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LNpCfaa8NfdY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eb0TUqkQNjaI"
      },
      "outputs": [],
      "source": [
        "\n",
        "import keras as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XJldwmiJ4hHA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Importar K e suas funÃ§Ãµes necessÃ¡rias\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MYZrHMd85PqQ"
      },
      "outputs": [],
      "source": [
        "# Inicializando a Rede Neural Convolucional\n",
        "classifier = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrympYJ96BWg",
        "outputId": "a2df101f-577f-41a8-9c65-f864817be1bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\art-g\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# 32 features; formato 3x3; Imagens 64x64; Array 3D (RGB).\n",
        "\n",
        "# Adicionando a Primeira Camada de ConvoluÃ§Ã£o\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape = (256, 256, 3), activation = 'relu'))\n",
        "\n",
        "# Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adicionando a Segunda Camada de ConvoluÃ§Ã£o\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "\n",
        "# Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adicionando a TERCEIRA Camada de ConvoluÃ§Ã£o\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "\n",
        "# Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Flattening - Transformando em 1D\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Full connection\n",
        "classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "# Compilando a rede\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1l-VgZ78NNS"
      },
      "source": [
        "**PrÃ©-processamento**\n",
        "\n",
        "Fazer prÃ©-processamento nos dados, em nosso caso as imagens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T8au9i8V7mDZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Criar o objeto com as regras de prÃ©-processamento\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=30,  # Adiciona rotaÃ§Ã£o\n",
        "    width_shift_range=0.1,  # Deslocamento horizontal\n",
        "    height_shift_range=0.1,  # Deslocamento vertical\n",
        "    fill_mode='nearest'  # Preencher pixels ausentes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLiOSL6h7vAI",
        "outputId": "5d332ac7-5f25-40df-e0a0-10d912e53ab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 14002 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# PrÃ©-processamento das imagens de treino\n",
        "training_set = train_datagen.flow_from_directory(r'G:\\Meu Drive\\.UFG\\Semestre Atual\\LIA\\Livros e Material\\3. Projeto TF Dogs Cats\\Dados\\dogs-vs-cats\\train',\n",
        "                                                 target_size = (256, 256),\n",
        "                                                 batch_size = 64,\n",
        "                                                 class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ephZE1Y59LKq"
      },
      "source": [
        "**Treinamento do Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KVRGudo9No6",
        "outputId": "e6557321-07e8-466f-b6be-c2dda104b3f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\art-g\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 4s/step - accuracy: 0.5427 - loss: 1.2185\n",
            "Epoch 2/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.4729 - loss: 0.7351\n",
            "Epoch 3/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5146 - loss: 0.6902\n",
            "Epoch 4/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.4999 - loss: 0.6947\n",
            "Epoch 5/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5359 - loss: 0.6864\n",
            "Epoch 6/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - accuracy: 0.5426 - loss: 0.6876\n",
            "Epoch 7/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5688 - loss: 0.6847\n",
            "Epoch 8/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6s/step - accuracy: 0.4907 - loss: 0.7086\n",
            "Epoch 9/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 5s/step - accuracy: 0.4938 - loss: 0.6881\n",
            "Epoch 10/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6s/step - accuracy: 0.5622 - loss: 0.6916\n",
            "Epoch 11/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4s/step - accuracy: 0.5579 - loss: 0.6897\n",
            "Epoch 12/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - accuracy: 0.5113 - loss: 0.6950\n",
            "Epoch 13/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5084 - loss: 0.6932\n",
            "Epoch 14/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - accuracy: 0.5885 - loss: 0.6913\n",
            "Epoch 15/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5757 - loss: 0.6895\n",
            "Epoch 16/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - accuracy: 0.5822 - loss: 0.6850\n",
            "Epoch 17/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4s/step - accuracy: 0.4984 - loss: 0.6898\n",
            "Epoch 18/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4s/step - accuracy: 0.5941 - loss: 0.6822\n",
            "Epoch 19/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5191 - loss: 0.6884\n",
            "Epoch 20/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5569 - loss: 0.6845\n",
            "Epoch 21/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5663 - loss: 0.6831\n",
            "Epoch 22/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4s/step - accuracy: 0.6058 - loss: 0.6818\n",
            "Epoch 23/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5194 - loss: 0.6938\n",
            "Epoch 24/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5374 - loss: 0.6845\n",
            "Epoch 25/25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4s/step - accuracy: 0.5415 - loss: 0.6849\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x1b8408c9990>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Executando o treinamento\n",
        "classifier.fit(training_set,\n",
        "               steps_per_epoch=5,\n",
        "               epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl1MFhzd9aph"
      },
      "source": [
        "Treinamento concluÃ­do com sucesso! ğŸ’ª Observe se ao final de cada Ã©poca a acurÃ¡cia aumenta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OIAQ0GU-WrK"
      },
      "source": [
        "**Teste do Modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQYol5tb-ejq"
      },
      "source": [
        "Testar o modelo treinado com imagens que ele ainda nÃ£o viu. Por fim, verificamos o resultado da previsÃ£o e emitimos a informaÃ§Ã£o se a imagem Ã© de um gato ou cachorro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "xthzuIX_IkU_",
        "outputId": "194a933a-4b6c-4fa6-a06a-c7486953e6c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "PrevisÃ£o: Cachorro.\n",
            "AcurÃ¡cia: 100.0 %.\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from IPython.display import Image\n",
        "\n",
        "# Carregando a imagem de teste\n",
        "test_image = image.load_img(r'G:\\Meu Drive\\.UFG\\Semestre Atual\\LIA\\Livros e Material\\3. Projeto TF Dogs Cats\\Dados\\dogs-vs-cats\\test\\86.jpg', target_size=(256, 256))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "# Fazendo a previsÃ£o usando o modelo classifier\n",
        "result = classifier.predict(test_image)\n",
        "class_indices = training_set.class_indices\n",
        "\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'Cachorro.'\n",
        "    accuracy = round(result[0][0] * 100, 2)  # A saÃ­da de previsÃ£o Ã© uma probabilidade entre 0 e 1\n",
        "else:\n",
        "    prediction = 'Gato.'\n",
        "    accuracy = round((1 - result[0][0]) * 100, 2)  # A saÃ­da de previsÃ£o Ã© uma probabilidade entre 0 e 1\n",
        "\n",
        "# Exibindo a previsÃ£o e a acurÃ¡cia\n",
        "print(\"PrevisÃ£o:\", prediction)\n",
        "print(\"AcurÃ¡cia:\", accuracy, \"%.\")\n",
        "\n",
        "# Exibindo a imagem\n",
        "Image(filename=r'G:\\Meu Drive\\.UFG\\Semestre Atual\\LIA\\Livros e Material\\3. Projeto TF Dogs Cats\\Dados\\dogs-vs-cats\\test\\86.jpg')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goRr2pHz-qrp"
      },
      "source": [
        "**O modelo recebeu uma imagem que nunca tinha visto antes e com base no que aprendeu durante o treinamento, foi capaz de classificar.**\n",
        "\n",
        "Convertemos a imagem de teste em um vetor de pixels e apresentamos ao modelo.\n",
        "O modelo compara o vetor da imagem de teste com seus pesos e entÃ£o emite a classificaÃ§Ã£o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfsCG3KP9kwr"
      },
      "source": [
        "**Melhorias adicionais para este modelo:**\n",
        "\n",
        "*   Aumentar o nÃºmero de Ã©pocas para 25 para uma aprendizagem mais profunda.\n",
        "*   aumentar o redimensionamento da imagem de 64x64 para 256x256.\n",
        "*   Aumentar o tamanho do lote de 32 para 64.\n",
        "*   Alterar a arquitetura da rede incluindo mais uma camada convolucional.\n",
        "*   Avaliar outras mÃ©tricas do modelo e ajustar os hiperparÃ¢metros de acordo.\n",
        "*   Experimentar outros algoritmos de otimizaÃ§Ã£o.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWz1StcE-MZz"
      },
      "source": [
        "Fim! ğŸ”¥"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
