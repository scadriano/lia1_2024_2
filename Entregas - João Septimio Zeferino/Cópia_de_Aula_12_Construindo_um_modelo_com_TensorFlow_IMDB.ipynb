{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OPzinn/lia1_2024_2/blob/main/Entregas%20-%20Jo%C3%A3o%20Septimio%20Zeferino/C%C3%B3pia_de_Aula_12_Construindo_um_modelo_com_TensorFlow_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Projeto Ponta a Ponta - Construindo um Modelo com Tensorflow -  IMDB**\n",
        "\n",
        "**Problema:** construir um modelo de Intelig√™ncia Artificial capaz de classificar reviews usando 10k palavras\n",
        "\n",
        "**N√£o h√° m√°gica. H√° matem√°tica!** üßô"
      ],
      "metadata": {
        "id": "Hu5OLDcwuSoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando o necess√°rio\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PMjHWncPAruw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carregando os dados de Treino e Teste e Definindo Parametros**\n"
      ],
      "metadata": {
        "id": "B80jFLJAym_v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wD9uqxNTtumI"
      },
      "outputs": [],
      "source": [
        "# Parametros IMDB\n",
        "max_words = 10000\n",
        "max_len = 500"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o dataset IMDB - J√° est√° no Keras!\n",
        "# Todos os DS no Keras -> https://keras.io/api/datasets/\n",
        "(x_treino, y_treino), (x_teste, y_teste) = datasets.imdb.load_data(num_words = max_words)"
      ],
      "metadata": {
        "id": "18IsYW3lA8I4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11892340-9007-4e5f-ca6b-2b47aa12d269"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_treino = pad_sequences(x_treino, maxlen = max_len)\n",
        "x_teste = pad_sequences(x_teste, maxlen = max_len)"
      ],
      "metadata": {
        "id": "tcS35L7rWQSX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Construindo a Rede Neural Convolucional**\n",
        "\n",
        "O Keras √© uma biblioteca do TensorFlow."
      ],
      "metadata": {
        "id": "eN2T6cv24Y6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_joao = Sequential()\n",
        "#camadas de classifica√ß√£o\n",
        "modelo_joao.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))  # Embedding para representar palavras\n",
        "modelo_joao.add(LSTM(64))  # LSTM para capturar depend√™ncias sequenciais\n",
        "modelo_joao.add(Dense(1, activation='sigmoid'))  # Sa√≠da bin√°ria"
      ],
      "metadata": {
        "id": "XJldwmiJ4hHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d57100cc-4e01-4de9-91d9-44a52b7ec913"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compila√ß√£o do modelo\n",
        "modelo_joao.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# adam - √© um algoritmo de aprendizagem tipo backpropagation!\n",
        "# loss - fun√ß√£o de erro. Isso se resume a uma otimiza√ß√£o fun√ß√£o matem√°tica!\n",
        "# metrics - medir o sucesso!"
      ],
      "metadata": {
        "id": "Ag6XmvRqCReR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Treinamento**"
      ],
      "metadata": {
        "id": "ephZE1Y59LKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Executando o treinamento\n",
        "%%time\n",
        "history = modelo_joao.fit(x_treino, y_treino, epochs=5, batch_size=64, validation_data=(x_teste, y_teste))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KVRGudo9No6",
        "outputId": "9832052d-29e4-492a-cbdc-abbb6b297f8b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 29ms/step - accuracy: 0.7179 - loss: 0.5371 - val_accuracy: 0.8192 - val_loss: 0.4094\n",
            "Epoch 2/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - accuracy: 0.8239 - loss: 0.3914 - val_accuracy: 0.7887 - val_loss: 0.4672\n",
            "Epoch 3/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.8791 - loss: 0.2981 - val_accuracy: 0.8734 - val_loss: 0.3134\n",
            "Epoch 4/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.9359 - loss: 0.1751 - val_accuracy: 0.8772 - val_loss: 0.3196\n",
            "Epoch 5/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.9493 - loss: 0.1399 - val_accuracy: 0.8645 - val_loss: 0.3587\n",
            "CPU times: user 1min, sys: 2.11 s, total: 1min 2s\n",
            "Wall time: 1min 30s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinamento conclu√≠do com sucesso! üí™ Observe se ao final de cada √©poca a acur√°cia aumenta."
      ],
      "metadata": {
        "id": "dl1MFhzd9aph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Previs√£o - Testar o modelo (Deploy)**"
      ],
      "metadata": {
        "id": "4OIAQ0GU-WrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testar o modelo com uma review para ver se ela √© positiva ou negativa.\n"
      ],
      "metadata": {
        "id": "KQYol5tb-ejq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# codigo para determinar positivo ou negativo\n",
        "def analise_filme(modelo_joao, review, word_index, max_words=10000, max_len=500):\n",
        "    # associa palavras ao token imdb\n",
        "    tokenizer = Tokenizer(num_words=max_words)\n",
        "    tokenizer.word_index = word_index\n",
        "\n",
        "    # conversao do texto para inteiros\n",
        "    sequences = tokenizer.texts_to_sequences([review])\n",
        "\n",
        "    # verifica se esta vazia\n",
        "    if len(sequences[0]) == 0:\n",
        "        return \"Review contains unknown words.\"\n",
        "\n",
        "    # padronizacao para compatibilidade com o modelo\n",
        "    padded_sequence = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "    # prever\n",
        "    prediction = modelo_joao.predict(padded_sequence)\n",
        "\n",
        "    # retorno de resultado\n",
        "    return 'Positiva' if prediction >= 0.5 else 'Negativa'\n",
        "\n",
        "# carregamento da database de palavras\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# exemplo de review de click 2006 para testar\n",
        "review = \"\"\"It's not the typical Adam Sandler movie and thank goodness for that. This movie has some actual drama,\n",
        "some real heart to it. It's not all lowbrow toilet humor. Has Adam Sandler grown up? Even in this more grownup venture\n",
        "he apparently just couldn't help himself, tossing in the obligatory disgusting fart joke. But we'll give him a pass on\n",
        "that one because pretty much everything else in the movie shows a refreshing maturity. Well, OK maybe not the humping\n",
        "dogs. But what do you want? Sandler's never going to go full-blown serious dramatist and who'd want him to? This movie\n",
        "maintains the humor Sandler is known for but also gives you a story you actually care about and moments of great emotion\n",
        "and poignancy. Along the way Sandler gets to show that he does have some actual serious acting chops. One scene with him\n",
        "and his father, played by Henry Winkler, particularly stands out. Here Sandler's character has so much emotion coursing\n",
        "through him. And Sandler performs the scene so well you feel the emotion right along with him. Very well done, and more\n",
        "than a little surprising from an actor who is not known for this sort of thing.\"\"\"\n",
        "\n",
        "# Fazer a previs√£o\n",
        "resultado = analise_filme(modelo_joao, review, word_index)\n",
        "print(f\"The review sentiment is: {resultado}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkNVjjAIbfWY",
        "outputId": "7afb7ee8-dd19-4555-f54e-03dcc9143658"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "The review sentiment is: Positiva\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fim! üî•"
      ],
      "metadata": {
        "id": "qWz1StcE-MZz"
      }
    }
  ]
}